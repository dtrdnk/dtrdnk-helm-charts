# Default values for tempo-operator.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
clusterName: cluster.local

replicaCount: 1

image:
  pullPolicy: IfNotPresent
  controllerManager:
    repository: ghcr.io/grafana/tempo-operator/tempo-operator
    tag: v0.4.0
  kubeRbacProxy:
    repository: gcr.io/kubebuilder/kube-rbac-proxy
    tag: v0.13.0

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

config: |
  apiVersion: config.tempo.grafana.com/v1alpha1
  kind: ProjectConfig
  distribution: community
  health:
    healthProbeBindAddress: :8081
  metrics:
    bindAddress: 127.0.0.1:8080
  webhook:
    port: 9443
  leaderElection:
    leaderElect: true
    resourceName: 8b886b0f.grafana.com
  # leaderElectionReleaseOnCancel defines if the leader should step down volume
  # when the Manager ends. This requires the binary to immediately end when the
  # Manager is stopped, otherwise, this setting is unsafe. Setting this significantly
  # speeds up voluntary leader transitions as the new leader don't have to wait
  # LeaseDuration time first.
  # In the default scaffold provided, the program ends immediately after
  # the manager stops, so would be fine to enable this option. However,
  # if you are doing or is intended to do any operation such as perform cleanups
  # after the manager stops then its usage might be unsafe.
  # leaderElectionReleaseOnCancel: true
  images:
    tempo: docker.io/grafana/tempo:2.1.1
    tempoQuery: docker.io/grafana/tempo-query:2.1.1
    tempoGateway: quay.io/observatorium/api:main-2023-07-31-5904ad0
    tempoGatewayOpa: quay.io/observatorium/opa-openshift:main-2023-05-24-8e91537
  featureGates:
    openshift:
      openshiftRoute: false
      servingCertsService: false
    prometheusOperator: false
    httpEncryption: false
    grpcEncryption: false
    tlsProfile: Modern
    builtInCertManagement:
      enabled: false
      # CA certificate validity: 5 years
      caValidity: 43830h
      # CA certificate refresh at 80% of validity
      caRefresh: 35064h
      # Target certificate validity: 90d
      certValidity: 2160h
      # Target certificate refresh at 80% of validity
      certRefresh: 1728h
    observability:
      metrics:
        createServiceMonitors: false
        createPrometheusRules: false

podAnnotations:
  kubectl.kubernetes.io/default-container: tempo-operator

securityContext:
  tempoOperator:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
  kubeRbacProxy:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL

podSecurityContext:
  runAsNonRoot: true

resources:
  tempoOperator:
    requests:
      cpu: 100m
      memory: 64Mi
  kubeRbacProxy:
    limits:
      cpu: 500m
      memory: 128Mi
    requests:
      cpu: 5m
      memory: 64Mi

service:
  webhook:
    port: 443

nodeSelector: {}

tolerations: []

affinity: {}
